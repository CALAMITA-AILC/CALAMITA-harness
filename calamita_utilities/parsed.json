{"tasks": [{"task_name": "veryfIT", "task_pretty_name": "veryfIT", "task_localURL": "VeryfIT", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "traceIT", "task_pretty_name": "trace-it", "task_localURL": "trace-it", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "perse", "task_pretty_name": "perseID", "task_localURL": "perseid", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "pejorativITy", "task_pretty_name": "pejorativITy", "task_localURL": "pejorativity", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "mult-it", "task_pretty_name": "multIT", "task_localURL": "mult-it", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "MAGNET", "task_pretty_name": "magnet", "task_localURL": "mt", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "itaeval", "task_pretty_name": "itaEval", "task_localURL": "itaeval", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "ita-sense", "task_pretty_name": "itaSense", "task_localURL": "ita-sense", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "invalsi", "task_pretty_name": "invalsi", "task_localURL": "invalsi", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "gita4calamita", "task_pretty_name": "gita4calamita", "task_localURL": "gita", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "gfg", "task_pretty_name": "gfg", "task_localURL": "gfg", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "geese", "task_pretty_name": "geese", "task_localURL": "geese", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "gattina", "task_pretty_name": "gattina", "task_localURL": "gattina", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "eurekaRebus", "task_pretty_name": "eurekaRebus", "task_localURL": "eurekarebus", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "ecwca", "task_pretty_name": "ecwca", "task_localURL": "ecwca", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "blm", "task_pretty_name": "blmIT", "task_localURL": "blm-it", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "beep", "task_pretty_name": "beep", "task_localURL": "beep", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "amelia", "task_pretty_name": "amelia", "task_localURL": "amelia", "hasPublicData": 0, "isTranslated": 0}, {"task_name": "abricot", "task_pretty_name": "abricot", "task_localURL": "abricot", "hasPublicData": 0, "isTranslated": 0}], "subtask": [{"subtask_name": "hellaswag_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.4532961561441944}, {"metric": "acc_norm", "result": 0.5792670782712607}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.5105556662019518}, {"metric": "acc_norm", "result": 0.6500697072296355}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5293766182035451}, {"metric": "acc_norm", "result": 0.6688906592312288}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.44811790479984065}, {"metric": "acc_norm", "result": 0.5581557458673571}]}]}, {"subtask_name": "haspeede2_stereo", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.6037287968283951}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.624441393100974}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.6743601018018004}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.46442426278669213}]}]}, {"subtask_name": "arc_easy_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.640993265993266}, {"metric": "acc_norm", "result": 0.5875420875420876}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.7171717171717171}, {"metric": "acc_norm", "result": 0.686026936026936}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.7622053872053872}, {"metric": "acc_norm", "result": 0.6948653198653199}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.6426767676767676}, {"metric": "acc_norm", "result": 0.5896464646464646}]}]}, {"subtask_name": "news_sum_fanpage", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "rouge1", "result": 0.32841168208870886}, {"metric": "rouge2", "result": 0.13540216809114486}, {"metric": "rougeL", "result": 0.22384366465675148}, {"metric": "bertscore", "result": 0.3759938573131497}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "rouge1", "result": 0.3121120102442282}, {"metric": "rouge2", "result": 0.1284379483066608}, {"metric": "rougeL", "result": 0.20824583254682125}, {"metric": "bertscore", "result": 0.3609682580391452}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "rouge1", "result": 0.29469811895911663}, {"metric": "rouge2", "result": 0.11557940479848541}, {"metric": "rougeL", "result": 0.20520838662889176}, {"metric": "bertscore", "result": 0.36535830938228464}]}]}, {"subtask_name": "news_sum_ilpost", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "rouge1", "result": 0.3200355367367411}, {"metric": "rouge2", "result": 0.13875761012115487}, {"metric": "rougeL", "result": 0.24010881687503788}, {"metric": "bertscore", "result": 0.3727420258422287}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "rouge1", "result": 0.27488369107967453}, {"metric": "rouge2", "result": 0.11847441540713886}, {"metric": "rougeL", "result": 0.20065476755520245}, {"metric": "bertscore", "result": 0.3316411314973084}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "rouge1", "result": 0.28447959330702144}, {"metric": "rouge2", "result": 0.11681924239996422}, {"metric": "rougeL", "result": 0.21897328387545067}, {"metric": "bertscore", "result": 0.3617828568738398}]}]}, {"subtask_name": "gfg_task_3_1", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "cwa", "result": 39.311901930808}, {"metric": "misgen-ratio", "result": 76.624058660325}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "cwa", "result": 35.68484230791005}, {"metric": "misgen-ratio", "result": 125.21530490912185}]}]}, {"subtask_name": "haspeede2_hs", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.6991982842062379}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.6998010187451045}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.7299554242949736}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.5158484971115631}]}]}, {"subtask_name": "ami_2020_aggressiveness", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.6043531519577608}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.48390898255900794}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.5242852385709529}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.43992832783003577}]}]}, {"subtask_name": "gfg_task_2_1", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc_gente", "result": 0.6830357142857143}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc_gente", "result": 0.6651785714285714}]}]}, {"subtask_name": "geese_noexp", "belongTo": "geese", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.4625}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.37625}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.2025}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.0975}]}]}, {"subtask_name": "ironita_sarcasm", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.5178314988006253}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.4641951625148186}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.6110748146036535}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.4168299104791996}]}]}, {"subtask_name": "geese_llama3", "belongTo": "geese", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.665}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.6225}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5675}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.55875}]}]}, {"subtask_name": "belebele_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.8577777777777778}, {"metric": "acc_norm", "result": 0.8577777777777778}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.8377777777777777}, {"metric": "acc_norm", "result": 0.8377777777777777}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.92}, {"metric": "acc_norm", "result": 0.92}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.42}, {"metric": "acc_norm", "result": 0.42}]}]}, {"subtask_name": "perse_task_3", "belongTo": "perse", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.49607149132715816}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.19760119940029985}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.5031568386639221}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.30770739360658955}]}]}, {"subtask_name": "perse_task_1", "belongTo": "perse", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.4973010010528603}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.24787574234810417}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.5035428968391803}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.2929607333424545}]}]}, {"subtask_name": "beep", "belongTo": "beep", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.6482876712328767}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.6243150684931507}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.8424657534246576}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5150684931506849}]}]}, {"subtask_name": "itacola", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.8246153846153846}, {"metric": "f1", "result": 0.6843076923076923}, {"metric": "mcc", "result": 0.369688904827535}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.6892307692307692}, {"metric": "f1", "result": 0.593049924994731}, {"metric": "mcc", "result": 0.2595791215947506}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.8769230769230769}, {"metric": "f1", "result": 0.7467664017453639}, {"metric": "mcc", "result": 0.4992063979481092}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.7435897435897436}, {"metric": "f1", "result": 0.6083053723629912}, {"metric": "mcc", "result": 0.23782977866190844}]}]}, {"subtask_name": "ecwca-hint", "belongTo": "ecwca", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "words_avg_f1", "result": 0.07088989441930618}, {"metric": "exact_match_custom", "result": 0.0}, {"metric": "edit_distance", "result": 393.6666666666667}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "words_avg_f1", "result": 0.6712164341967929}, {"metric": "exact_match_custom", "result": 0.6165670367207515}, {"metric": "edit_distance", "result": 4.9444918872758326}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "words_avg_f1", "result": 0.5206010328982148}, {"metric": "exact_match_custom", "result": 0.47907771135781385}, {"metric": "edit_distance", "result": 5.415029888983774}]}]}, {"subtask_name": "ecwca-no-hint", "belongTo": "ecwca", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "words_avg_f1", "result": 0.030303030303030304}, {"metric": "exact_match_custom", "result": 0.0}, {"metric": "edit_distance", "result": 317.6666666666667}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "words_avg_f1", "result": 0.6599720518593274}, {"metric": "exact_match_custom", "result": 0.6319385140905209}, {"metric": "edit_distance", "result": 11.216908625106747}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "words_avg_f1", "result": 0.5364747265259647}, {"metric": "exact_match_custom", "result": 0.4987190435525192}, {"metric": "edit_distance", "result": 5.115286080273271}]}]}, {"subtask_name": "arc_challenge_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.4112627986348123}, {"metric": "acc_norm", "result": 0.4274744027303754}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.5341296928327645}, {"metric": "acc_norm", "result": 0.5631399317406144}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5264505119453925}, {"metric": "acc_norm", "result": 0.5426621160409556}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.3720136518771331}, {"metric": "acc_norm", "result": 0.3984641638225256}]}]}, {"subtask_name": "eureka_original", "belongTo": "eurekaRebus", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "word_guesses_accuracy", "result": 0.11813094411114648}, {"metric": "first_pass_accuracy", "result": 0.0031575623618566467}, {"metric": "solution_words_accuracy", "result": 0.01618162298705401}, {"metric": "solution_words_lengths_accuracy", "result": 0.16426141458793841}, {"metric": "solution_match", "result": 0.00031575623618566466}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "word_guesses_accuracy", "result": 0.001826081465108936}, {"metric": "first_pass_accuracy", "result": 0.0}, {"metric": "solution_words_accuracy", "result": 6.315124723713294e-05}, {"metric": "solution_words_lengths_accuracy", "result": 0.008610230502052415}, {"metric": "solution_match", "result": 0.0}]}]}, {"subtask_name": "ironita_irony", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.6741186970102633}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.6869456865349934}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.7869767841419153}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.4205613391133753}]}]}, {"subtask_name": "perse_task_2", "belongTo": "perse", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.49703006640638037}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.26859912052409585}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.5090304649728773}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.27316356864009167}]}]}, {"subtask_name": "gfg_task_1_2", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "bert_f1", "result": 0.48775068490182527}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "bert_f1", "result": 0.287073515599058}]}]}, {"subtask_name": "ami_2020_misogyny", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.7264848403202169}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.7272727272727273}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.850897235957139}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.5155038759689923}]}]}, {"subtask_name": "perse_task_0", "belongTo": "perse", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.4953869281153722}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.3238903867047604}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.5050071278973582}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.2151840362203404}]}]}, {"subtask_name": "hatecheck_ita", "belongTo": "hatecheck_ita", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.825468982535432}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.8117266439604165}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.894126056858292}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.7090807314023166}]}]}, {"subtask_name": "gfg_task_1_1", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "bert_f1", "result": 0.6407975602217856}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "bert_f1", "result": 0.4746150076389313}]}]}, {"subtask_name": "ita-sense-ml-no-translation", "belongTo": "ita-sense", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "extract_answer", "result": 0.41268803139306737}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "extract_answer", "result": 0.5120994113799869}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "extract_answer", "result": 0.6285153695225638}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "extract_answer", "result": 0.21517331589274036}]}]}, {"subtask_name": "ita-sense-gen-no-translation", "belongTo": "ita-sense", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "rougeBertScore", "result": 0.31921862236515036}, {"metric": "rougeL", "result": 0.13560427055194377}, {"metric": "bertScore", "result": 0.6945834092159009}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "rougeBertScore", "result": 0.25854083584126464}, {"metric": "rougeL", "result": 0.11461734279239567}, {"metric": "bertScore", "result": 0.6954457179486787}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "rougeBertScore", "result": 0.3143932444909845}, {"metric": "rougeL", "result": 0.14036341437963218}, {"metric": "bertScore", "result": 0.6977120226040902}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "rougeBertScore", "result": 0.25691417126109845}, {"metric": "rougeL", "result": 0.09618504813913363}, {"metric": "bertScore", "result": 0.6663492498323722}]}]}, {"subtask_name": "ita-sense-ml-with-translation", "belongTo": "ita-sense", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "extract_answer", "result": 0.3856280855732309}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "extract_answer", "result": 0.4750411409764125}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "extract_answer", "result": 0.5787164015359297}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "extract_answer", "result": 0.2002194185408667}]}]}, {"subtask_name": "ita-sense-gen-with-translation", "belongTo": "ita-sense", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "rougeBertScore", "result": 0.3192784120872682}, {"metric": "rougeL", "result": 0.13575727255643388}, {"metric": "bertScore", "result": 0.6930439857671321}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "rougeBertScore", "result": 0.2630987442843378}, {"metric": "rougeL", "result": 0.11698102831697799}, {"metric": "bertScore", "result": 0.6940515559734934}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "rougeBertScore", "result": 0.30995102685633047}, {"metric": "rougeL", "result": 0.13811661065382028}, {"metric": "bertScore", "result": 0.6952376224858275}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "rougeBertScore", "result": 0.2549140849172947}, {"metric": "rougeL", "result": 0.09494739951914671}, {"metric": "bertScore", "result": 0.6646080478646997}]}]}, {"subtask_name": "geese_dummy", "belongTo": "geese", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.48625}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.50375}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.235}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.43875}]}]}, {"subtask_name": "gfg_task_2_2", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "cwa", "result": 52.712756922031616}, {"metric": "misgen-ratio", "result": 51.69681784723403}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "cwa", "result": 53.89955268671079}, {"metric": "misgen-ratio", "result": 108.85595379650076}]}]}, {"subtask_name": "geese_human", "belongTo": "geese", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.5425}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc", "result": 0.56375}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.54875}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.44375}]}]}, {"subtask_name": "gfg_task_2_3", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc_gente", "result": 0.31466666666666665}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc_gente", "result": 0.49866666666666665}]}]}, {"subtask_name": "sentipolc", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "f1", "result": 0.49026211686349636}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.5042007173387677}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.5741440758150925}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.44031286139661147}]}]}, {"subtask_name": "gfg_task_3_2", "belongTo": "gfg", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc_gente", "result": 0.6093333333333333}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "acc_gente", "result": 0.566}]}]}, {"subtask_name": "veryfIT_small", "belongTo": "veryfIT", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.5227272727272727}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5568181818181818}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5681818181818182}]}]}, {"subtask_name": "veryfIT_enriched", "belongTo": "veryfIT", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.5227272727272727}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5568181818181818}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5681818181818182}]}]}, {"subtask_name": "veryfIT_full", "belongTo": "veryfIT", "evaluations": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "metrics": [{"metric": "acc", "result": 0.5248065543923532}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5932706580900544}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5922810489856507}]}]}, {"subtask_name": "amelia-arg-component-fewshot", "belongTo": "amelia", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "macro_f1_score", "result": 0.36074295103553405}, {"metric": "f1_classes", "result": "[0.23239437 0.         0.84983449]"}, {"metric": "accuracy", "result": 0.6854838709677419}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "macro_f1_score", "result": 0.5559765392771897}, {"metric": "f1_classes", "result": "[0.70378619 0.         0.96414343]"}, {"metric": "accuracy", "result": 0.9359582542694497}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "macro_f1_score", "result": 0.3020560172762613}, {"metric": "f1_classes", "result": "[0.00823045 0.         0.8979376 ]"}, {"metric": "accuracy", "result": 0.8059772296015181}]}]}, {"subtask_name": "amelia-arg-component-zeroshot", "belongTo": "amelia", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "macro_f1_score", "result": 0.3710655255711457}, {"metric": "f1_classes", "result": "[0.17554859 0.         0.93764799]"}, {"metric": "accuracy", "result": 0.8586337760910816}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "macro_f1_score", "result": 0.46811370596348506}, {"metric": "f1_classes", "result": "[0.45876289 0.         0.94557823]"}, {"metric": "accuracy", "result": 0.8994307400379506}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "macro_f1_score", "result": 0.29390102097742293}, {"metric": "f1_classes", "result": "[0.02214022 0.         0.85956284]"}, {"metric": "accuracy", "result": 0.7476280834914611}]}]}, {"subtask_name": "amelia-arg-premisetype-fewshot", "belongTo": "amelia", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "macro_f1_score", "result": 0.7977681524568103}, {"metric": "f1_classes", "result": "[0.82072307 0.77481323]"}, {"metric": "MC_accuracy", "result": 0.590032154340836}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "macro_f1_score", "result": 0.8639003844248998}, {"metric": "f1_classes", "result": "[0.89199689 0.83580388]"}, {"metric": "MC_accuracy", "result": 0.7427652733118971}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "macro_f1_score", "result": 0.13149989394518763}, {"metric": "f1_classes", "result": "[0.18026969 0.08273009]"}, {"metric": "MC_accuracy", "result": 0.006430868167202572}]}]}, {"subtask_name": "amelia-arg-premisetype-zeroshot", "belongTo": "amelia", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "macro_f1_score", "result": 0.7179995142148419}, {"metric": "f1_classes", "result": "[0.76751707 0.66848196]"}, {"metric": "MC_accuracy", "result": 0.5139335476956056}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "macro_f1_score", "result": 0.8477194648865058}, {"metric": "f1_classes", "result": "[0.87260428 0.82283465]"}, {"metric": "MC_accuracy", "result": 0.7518756698821008}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "macro_f1_score", "result": 0.36315982617603104}, {"metric": "f1_classes", "result": "[0.50914794 0.21717172]"}, {"metric": "MC_accuracy", "result": 0.11361200428724544}]}]}, {"subtask_name": "amelia-arg-scheme-fewshot", "belongTo": "amelia", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "macro_f1_score", "result": 0.4978142509303168}, {"metric": "f1_classes", "result": "[0.28971963 0.39405204 0.77824268 0.28503563 0.74202128]"}, {"metric": "MC_accuracy", "result": 0.26092384519350814}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "macro_f1_score", "result": 0.5474381202916134}, {"metric": "f1_classes", "result": "[0.38461538 0.48623853 0.81037277 0.24291498 0.81304893]"}, {"metric": "MC_accuracy", "result": 0.41697877652933835}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "macro_f1_score", "result": 0.26664601004406513}, {"metric": "f1_classes", "result": "[0.11081081 0.37677985 0.48268625 0.1974359  0.16551724]"}, {"metric": "MC_accuracy", "result": 0.0}]}]}, {"subtask_name": "amelia-arg-scheme-zeroshot", "belongTo": "amelia", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "macro_f1_score", "result": 0.36222599578439113}, {"metric": "f1_classes", "result": "[0.30357143 0.05577689 0.57575758 0.27906977 0.59695431]"}, {"metric": "MC_accuracy", "result": 0.22971285892634208}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "macro_f1_score", "result": 0.5047523488159766}, {"metric": "f1_classes", "result": "[0.45652174 0.14229249 0.82033898 0.33846154 0.76614699]"}, {"metric": "MC_accuracy", "result": 0.44069912609238454}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "macro_f1_score", "result": 0.05656657068421774}, {"metric": "f1_classes", "result": "[0.1        0.07272727 0.05128205 0.         0.05882353]"}, {"metric": "MC_accuracy", "result": 0.00749063670411985}]}]}, {"subtask_name": "blm_od1_0shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.05625995517800767}]}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.057766086539098815}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.07175157171929893}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.025985663082437275}]}]}, {"subtask_name": "blm_caus1_0shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.04763617241010421}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.08695583378236851}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.0250787224471435}]}]}, {"subtask_name": "blm_caus2_0shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.06715288355895183}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.08416135678620956}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.028956675508399646}]}]}, {"subtask_name": "blm_caus1_1shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.08176935719678124}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.17861693160609884}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.09163281318964486}]}]}, {"subtask_name": "blm_caus2_1shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.12259124085699119}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.19176542565629234}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.08764562219382355}]}]}, {"subtask_name": "blm_od2_0shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.05662657526892521}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.06920363632197575}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.03021978021978022}]}]}, {"subtask_name": "blm_od1_1shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.08686878355214397}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.27212855272745085}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.08789439982762329}]}]}, {"subtask_name": "blm_od2_1shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.10120225478502334}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.1964204396140406}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.09658146726550293}]}]}, {"subtask_name": "blm_agr1_0shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.1405335507256354}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.3275875260473141}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.027084262148907714}]}]}, {"subtask_name": "blm_agr2_0shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.19820170728673997}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.34962815523031004}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.024164408310749775}]}]}, {"subtask_name": "blm_agr1_1shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.18409660865675453}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.40752006870333946}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.07388783618025155}]}]}, {"subtask_name": "blm_agr2_1shots", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": "f1", "result": 0.2988161930960832}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "f1", "result": 0.40983671165242236}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "f1", "result": 0.09055892468167225}]}]}, {"subtask_name": "agr_tasks", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": " ", "result": " "}]}]}, {"subtask_name": "od_tasks", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": " ", "result": " "}]}]}, {"subtask_name": "caus_tasks", "belongTo": "blm", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": [{"metric": " ", "result": " "}]}]}, {"subtask_name": "abricot_abs", "belongTo": "abricot", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": []}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "pearson", "result": 0.44144492353948517}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "pearson", "result": -0.027356569174928617}]}]}, {"subtask_name": "abricot_inc", "belongTo": "abricot", "evaluations": [{"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "metrics": []}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "pearson", "result": 0.2504642797979997}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "pearson", "result": -0.1979970181204069}]}]}, {"subtask_name": "honest_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.0}]}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.0}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.0}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.0}]}]}, {"subtask_name": "conflict_detect", "belongTo": "gita4calamita", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.6317991631799164}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.2301255230125523}]}]}, {"subtask_name": "gattina-ansa", "belongTo": "gattina", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "sbert_score", "result": 0.5946770935604074}, {"metric": "ns_score", "result": 0.3015605361786334}, {"metric": "ha_score", "result": 0.6695801243024445}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "sbert_score", "result": 0.06594011779089921}, {"metric": "ns_score", "result": 0.9909901712308193}, {"metric": "ha_score", "result": 0.022744434603336767}]}]}, {"subtask_name": "gattina-galileo", "belongTo": "gattina", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "sbert_score", "result": 0.2602965398416212}, {"metric": "ns_score", "result": 0.727192985939315}, {"metric": "ha_score", "result": 0.21006301597926325}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "sbert_score", "result": 0.2079737666160007}, {"metric": "ns_score", "result": 0.7919537439475746}, {"metric": "ha_score", "result": 0.11773700427625133}]}]}, {"subtask_name": "invalsi_ita", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.8871978513876455}, {"metric": "acc_norm", "result": 0.8809310653536258}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.37958818263205013}, {"metric": "acc_norm", "result": 0.36705461056401073}]}]}, {"subtask_name": "invalsi_ita_binarie", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.7428571428571429}, {"metric": "acc_norm", "result": 0.6928571428571428}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5928571428571429}, {"metric": "acc_norm", "result": 0.4928571428571429}]}]}, {"subtask_name": "invalsi_ita_multipla", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.9078812691914022}, {"metric": "acc_norm", "result": 0.9078812691914022}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.3490276356192426}, {"metric": "acc_norm", "result": 0.3490276356192426}]}]}, {"subtask_name": "invalsi_mate", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.7175}, {"metric": "acc_norm", "result": 0.7325}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.335}, {"metric": "acc_norm", "result": 0.345}]}]}, {"subtask_name": "invalsi_mate_multipla", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.7049180327868853}, {"metric": "acc_norm", "result": 0.7049180327868853}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.30327868852459017}, {"metric": "acc_norm", "result": 0.30327868852459017}]}]}, {"subtask_name": "invalsi_mate_numero", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.7843137254901961}, {"metric": "acc_norm", "result": 0.8529411764705882}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.27450980392156865}, {"metric": "acc_norm", "result": 0.38235294117647056}]}]}, {"subtask_name": "invalsi_mate_verofalso", "belongTo": "invalsi", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.6481481481481481}, {"metric": "acc_norm", "result": 0.6296296296296297}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5925925925925926}, {"metric": "acc_norm", "result": 0.46296296296296297}]}]}, {"subtask_name": "multi-it-a", "belongTo": "mult-it", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.8398345153664303}, {"metric": "f1", "result": 0.8175311144314964}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.39184397163120566}, {"metric": "f1", "result": 0.39277188977421673}]}]}, {"subtask_name": "multi-it-c", "belongTo": "mult-it", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.8121356455854497}, {"metric": "f1", "result": 0.6634600110098486}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.5030159810213877}, {"metric": "f1", "result": 0.3892927632509022}]}]}, {"subtask_name": "pejorativITy-misoginy", "belongTo": "pejorativITy", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.75}, {"metric": "f1", "result": 0.7328552431610942}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.6075}, {"metric": "f1", "result": 0.5703178750652848}]}]}, {"subtask_name": "pejorativITy-misoginy-context", "belongTo": "pejorativITy", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.7991666666666667}, {"metric": "f1", "result": 0.793250028059946}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.6591666666666667}, {"metric": "f1", "result": 0.4042062456374617}]}]}, {"subtask_name": "pejorativITy-standard", "belongTo": "pejorativITy", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5916666666666667}, {"metric": "f1", "result": 0.5334162815499467}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.43333333333333335}, {"metric": "f1", "result": 0.36329389292725633}]}]}, {"subtask_name": "physical_state", "belongTo": "gita4calamita", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.40336134453781514}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.12184873949579832}]}]}, {"subtask_name": "squad_it", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "squad_em", "result": 0.6588250755684059}, {"metric": "squad_f1", "result": 0.7817207639226809}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "squad_em", "result": 0.0017085030884478906}, {"metric": "squad_f1", "result": 0.12548220679303676}]}]}, {"subtask_name": "story_class", "belongTo": "gita4calamita", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.4747191011235955}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.33146067415730335}]}]}, {"subtask_name": "traceIT", "belongTo": "traceIT", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.8498233215547704}, {"metric": "f1", "result": 0.8423005565862709}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.6289752650176679}, {"metric": "f1", "result": 0.53125}]}]}, {"subtask_name": "truthfulqa_gen_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "bleu_max", "result": 40.283175332834766}, {"metric": "bleu_acc", "result": 0.5716034271725826}, {"metric": "bleu_diff", "result": 17.979788374477604}, {"metric": "rouge1_max", "result": 60.024910209877596}, {"metric": "rouge1_acc", "result": 0.5997552019583844}, {"metric": "rouge1_diff", "result": 19.39552322472658}, {"metric": "rouge2_max", "result": 46.8211017601364}, {"metric": "rouge2_acc", "result": 0.5397796817625459}, {"metric": "rouge2_diff", "result": 18.732398954311556}, {"metric": "rougeL_max", "result": 57.896656706961465}, {"metric": "rougeL_acc", "result": 0.572827417380661}, {"metric": "rougeL_diff", "result": 19.036011695576125}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "bleu_max", "result": 16.476549822554265}, {"metric": "bleu_acc", "result": 0.3574051407588739}, {"metric": "bleu_diff", "result": -3.0833687822705937}, {"metric": "rouge1_max", "result": 38.552305186867684}, {"metric": "rouge1_acc", "result": 0.37576499388004897}, {"metric": "rouge1_diff", "result": -5.400378110126922}, {"metric": "rouge2_max", "result": 23.263856120454914}, {"metric": "rouge2_acc", "result": 0.3011015911872705}, {"metric": "rouge2_diff", "result": -6.030441272543846}, {"metric": "rougeL_max", "result": 35.65988570210397}, {"metric": "rougeL_acc", "result": 0.3623011015911873}, {"metric": "rougeL_diff", "result": -5.834126213168319}]}]}, {"subtask_name": "truthfulqa_mc1_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.37576499388004897}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.24479804161566707}]}]}, {"subtask_name": "truthfulqa_mc2_ita", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.5388943664216962}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.4095566626631053}]}]}, {"subtask_name": "gente_rephrasing", "belongTo": "itaeval", "evaluations": [{"model_name": "meta-llama/Llama-3.1-70B-Instruct", "metrics": [{"metric": "acc", "result": 0.4268456375838926}]}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "acc", "result": 0.2563758389261745}]}]}, {"subtask_name": "eureka_hints", "belongTo": "eurekaRebus", "evaluations": [{"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "metrics": [{"metric": "word_guesses_accuracy", "result": 0.0020313230186296185}, {"metric": "first_pass_accuracy", "result": 0.0}, {"metric": "solution_words_accuracy", "result": 0.00011578781180928324}, {"metric": "solution_words_lengths_accuracy", "result": 0.002462835491000947}, {"metric": "solution_match", "result": 0.0}]}]}], "model": [{"model_name": "meta-llama/Llama-3.1-8B-Instruct", "model_pretty_name": "", "model_URL": ""}, {"model_name": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA", "model_pretty_name": "", "model_URL": ""}, {"model_name": "meta-llama/Llama-3.1-70B-Instruct", "model_pretty_name": "", "model_URL": ""}, {"model_name": "sapienzanlp/Minerva-7B-instruct-v1.0", "model_pretty_name": "", "model_URL": ""}]}
